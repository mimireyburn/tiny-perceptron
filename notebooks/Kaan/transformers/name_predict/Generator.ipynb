{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /root/.venv/lib/python3.10/site-packages (0.1.99)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: names_1.txt\n",
      "  input_format: \n",
      "  model_prefix: m\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 200\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: names_1.txt\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 32033 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=548476\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9504% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=28\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999504\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 32033 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=279233\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 23501 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 32033\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 29494\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 29494 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=14220 obj=40.0911 num_tokens=259682 num_tokens/piece=18.2617\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=12689 obj=29.7965 num_tokens=259627 num_tokens/piece=20.4608\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=9512 obj=29.6966 num_tokens=261642 num_tokens/piece=27.5065\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=9459 obj=29.5189 num_tokens=261650 num_tokens/piece=27.6615\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=7094 obj=29.8576 num_tokens=264161 num_tokens/piece=37.2372\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=7093 obj=29.7122 num_tokens=264160 num_tokens/piece=37.2424\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=5319 obj=30.097 num_tokens=266677 num_tokens/piece=50.1367\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=5319 obj=29.9458 num_tokens=266673 num_tokens/piece=50.1359\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=3989 obj=30.3348 num_tokens=268997 num_tokens/piece=67.4347\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=3989 obj=30.2036 num_tokens=268992 num_tokens/piece=67.4334\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2991 obj=30.5879 num_tokens=271328 num_tokens/piece=90.7148\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2991 obj=30.4693 num_tokens=271321 num_tokens/piece=90.7125\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2243 obj=30.852 num_tokens=273604 num_tokens/piece=121.981\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2243 obj=30.752 num_tokens=273601 num_tokens/piece=121.98\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=1682 obj=31.1305 num_tokens=275967 num_tokens/piece=164.071\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=1682 obj=31.026 num_tokens=275966 num_tokens/piece=164.07\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=1261 obj=31.4359 num_tokens=278664 num_tokens/piece=220.987\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=1261 obj=31.3195 num_tokens=278665 num_tokens/piece=220.987\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=945 obj=31.7319 num_tokens=281646 num_tokens/piece=298.038\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=945 obj=31.5983 num_tokens=281646 num_tokens/piece=298.038\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=708 obj=32.0507 num_tokens=285049 num_tokens/piece=402.612\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=708 obj=31.9 num_tokens=285050 num_tokens/piece=402.613\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=531 obj=32.3856 num_tokens=289140 num_tokens/piece=544.52\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=531 obj=32.2109 num_tokens=289150 num_tokens/piece=544.539\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=398 obj=32.7489 num_tokens=293693 num_tokens/piece=737.922\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=398 obj=32.5429 num_tokens=293694 num_tokens/piece=737.925\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=298 obj=33.115 num_tokens=298726 num_tokens/piece=1002.44\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=298 obj=32.8825 num_tokens=298728 num_tokens/piece=1002.44\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=223 obj=33.5082 num_tokens=304026 num_tokens/piece=1363.35\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=223 obj=33.2606 num_tokens=304026 num_tokens/piece=1363.35\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=220 obj=33.2842 num_tokens=304227 num_tokens/piece=1382.85\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=220 obj=33.2732 num_tokens=304227 num_tokens/piece=1382.85\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: m.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: m.vocab\n",
      "6 num_tokens=264161 num_tokens/piece=37.2372\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=7093 obj=29.7122 num_tokens=264160 num_tokens/piece=37.2424\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=5319 obj=30.097 num_tokens=266677 num_tokens/piece=50.1367\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=5319 obj=29.9458 num_tokens=266673 num_tokens/piece=50.1359\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=3989 obj=30.3348 num_tokens=268997 num_tokens/piece=67.4347\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=3989 obj=30.2036 num_tokens=268992 num_tokens/piece=67.4334\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2991 obj=30.5879 num_tokens=271328 num_tokens/piece=90.7148\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2991 obj=30.4693 num_tokens=271321 num_tokens/piece=90.7125\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2243 obj=30.852 num_tokens=273604 num_tokens/piece=121.981\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2243 obj=30.752 num_tokens=273601 num_tokens/piece=121.98\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=1682 obj=31.1305 num_tokens=275967 num_tokens/piece=164.071\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=1682 obj=31.026 num_tokens=275966 num_tokens/piece=164.07\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=1261 obj=31.4359 num_tokens=278664 num_tokens/piece=220.987\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=1261 obj=31.3195 num_tokens=278665 num_tokens/piece=220.987\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=945 obj=31.7319 num_tokens=281646 num_tokens/piece=298.038\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=945 obj=31.5983 num_tokens=281646 num_tokens/piece=298.038\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=708 obj=32.0507 num_tokens=285049 num_tokens/piece=402.612\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=708 obj=31.9 num_tokens=285050 num_tokens/piece=402.613\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=531 obj=32.3856 num_tokens=289140 num_tokens/piece=544.52\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=531 obj=32.2109 num_tokens=289150 num_tokens/piece=544.539\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=398 obj=32.7489 num_tokens=293693 num_tokens/piece=737.922\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=398 obj=32.5429 num_tokens=293694 num_tokens/piece=737.925\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=298 obj=33.115 num_tokens=298726 num_tokens/piece=1002.44\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=298 obj=32.8825 num_tokens=298728 num_tokens/piece=1002.44\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=223 obj=33.5082 num_tokens=304026 num_tokens/piece=1363.35\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=223 obj=33.2606 num_tokens=304026 num_tokens/piece=1363.35\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=220 obj=33.2842 num_tokens=304227 num_tokens/piece=1382.85\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=220 obj=33.2732 num_tokens=304227 num_tokens/piece=1382.85\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: m.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: m.vocab\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece\n",
    "\n",
    "import sentencepiece as spm\n",
    "import random\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "input_file = 'names_1.txt'\n",
    "prefix = 'm'\n",
    "vocab_size = 200\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=input_file, \n",
    "    model_prefix=prefix, \n",
    "    vocab_size=vocab_size\n",
    ")\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "        self.sp.load(f'{prefix}.model')\n",
    "        self.vocab_size = self.sp.get_piece_size()\n",
    "\n",
    "    def encode(self, name):\n",
    "        return self.sp.encode_as_ids(name)\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return self.sp.decode_ids(tokens)\n",
    "    \n",
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('names.txt', 'r') as f:\n",
    "    names = f.readlines()\n",
    "\n",
    "# Add <sos> and <eos> tokens to each name\n",
    "formatted_names = ['<sos>' + name.strip() + '<eos>\\n' for name in names]\n",
    "\n",
    "# Write the formatted names to a new file\n",
    "with open('names_1.txt', 'w') as f:\n",
    "    f.writelines(formatted_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ke\n",
      "[7, 78, 41, 14]\n",
      " ⁇    < > sos eos  e a s y i ri n t an d o na ma g ra l u la el k ja ni ka re mi da z on c li le b m h sa ar ya en ne vi f ta ah er p w za in de ha x sh al r ro th lyn di ca lo me ke or se ce am ly si lee st jo ley ch ana lynn mar ay sha ry ia ad lin is va il lan em us anna ti ky ai "
     ]
    }
   ],
   "source": [
    "\n",
    "# print(tokenizer.vocab_size)  # 29\n",
    "foo = tokenizer.encode('john') # [2, 12, 17, 10, 16, 1]\n",
    "bar = tokenizer.decode([69])    # john\n",
    "print(bar)\n",
    "print(foo)\n",
    "for i in range(100):\n",
    "    print(tokenizer.decode([i]), end=' ')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "  def __init__(self):\n",
    "    with open('names_1.txt', 'r') as f:\n",
    "      self.names = f.read().split('\\n')\n",
    "    self.tokenizer = Tokenizer()\n",
    "\n",
    "  def __len__(self):\n",
    "    # Return number of names\n",
    "    return len(self.names)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    # Get name at index\n",
    "    name = self.names[idx]\n",
    "    # Return encoded name\n",
    "    return torch.tensor(self.tokenizer.encode(name))\n",
    "\n",
    "\n",
    "ds = Dataset()\n",
    "dl = torch.utils.data.DataLoader(ds, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "class BesSimpleTransformer(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(BesSimpleTransformer, self).__init__()\n",
    "    # Embedding part of the model - 7 is the embedding size\n",
    "    self.embedding    = torch.nn.Embedding(tokenizer.vocab_size, 7)\n",
    "    self.pos_emb      = self.get_pos_matrix()\n",
    "    # Mask tensor trick - if batch size is one, we might not need it - research it!\n",
    "    self.register_buffer('mask', torch.tril(torch.ones(19, 19)))\n",
    "    # First decoder block\n",
    "    # 11 could be anything, if we have heads or batch_size this might change\n",
    "    self.layer_00_key = torch.nn.Linear(7, 11)\n",
    "    self.layer_00_qry = torch.nn.Linear(7, 11)\n",
    "    self.layer_00_val = torch.nn.Linear(7, 11)\n",
    "    self.layer_00_ffw = torch.nn.Linear(11, 7)\n",
    "    # Second decoder block\n",
    "    self.layer_01_key = torch.nn.Linear(7, 11)\n",
    "    self.layer_01_qry = torch.nn.Linear(7, 11)\n",
    "    self.layer_01_val = torch.nn.Linear(7, 11)\n",
    "    self.layer_01_ffw = torch.nn.Linear(11, 7)\n",
    "    # Output of the model\n",
    "    self.map_to_vocab = torch.nn.Linear(7, tokenizer.vocab_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    emb = self.embedding(x)\n",
    "    pos = self.pos_emb[0:x.shape[0], :]\n",
    "    emb = emb + pos\n",
    "\n",
    "    key = self.layer_00_key(emb)\n",
    "    qry = self.layer_00_qry(emb)\n",
    "    val = self.layer_00_val(emb)\n",
    "    att = torch.mm(qry, key.t())\n",
    "    # mask from 0 to token end (square mask)\n",
    "    msk = self.mask[0:x.shape[0], 0:x.shape[0]]\n",
    "    # mask over tensor (same as adding it)\n",
    "    att = att.masked_fill(msk == 0, float('-inf'))\n",
    "    att_00 = torch.nn.functional.softmax(att, dim=1)\n",
    "    att = torch.nn.functional.softmax(att, dim=1)\n",
    "    res = torch.mm(att, val)\n",
    "    # this is the feed forward layer\n",
    "    res = self.layer_00_ffw(res)\n",
    "\n",
    "    # do it all again with new q, k, v\n",
    "    key = self.layer_01_key(res)\n",
    "    qry = self.layer_01_qry(res)\n",
    "    val = self.layer_01_val(res)\n",
    "    att = torch.mm(qry, key.t())\n",
    "    msk = self.mask[0:x.shape[0], 0:x.shape[0]]\n",
    "    att = att.masked_fill(msk == 0, float('-inf'))\n",
    "    att_01 = torch.nn.functional.softmax(att, dim=1)\n",
    "    att = torch.nn.functional.softmax(att, dim=1)\n",
    "    res = torch.mm(att, val)\n",
    "    res = self.layer_01_ffw(res)\n",
    "\n",
    "    # map back to our 29 vocab (alphabet + pos, eos, sos)\n",
    "    out = self.map_to_vocab(res)\n",
    "    return out, [att_00, att_01]\n",
    "\n",
    "  def get_pos_matrix(self):\n",
    "    store = torch.zeros(19, 7)\n",
    "    for pos in range(19):\n",
    "      # why do we do this range thing\n",
    "      for i in range(0, 7, 2):\n",
    "        denominator = 10000 ** (2 * i / 7)\n",
    "        store[pos, i] = math.sin(pos / denominator)\n",
    "        if i + 1 < 7: store[pos, i + 1] = math.cos(pos / denominator)\n",
    "    return store\n",
    "\n",
    "\n",
    "m = BesSimpleTransformer()\n",
    "\n",
    "# SDG instead of Adam, why?\n",
    "opt = torch.optim.Adam(m.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Loss: 1.18683922290802\n",
      "Epoch: 0\n",
      "Loss: 1.6655734777450562\n",
      "Epoch: 0\n",
      "Loss: 1.5813531875610352\n",
      "Epoch: 0\n",
      "Loss: 2.0145668983459473\n",
      "Epoch: 0\n",
      "Loss: 1.6702752113342285\n",
      "Epoch: 0\n",
      "Loss: 2.1207222938537598\n",
      "Epoch: 0\n",
      "Loss: 1.3382995128631592\n",
      "Epoch: 0\n",
      "Loss: 1.5434716939926147\n",
      "Epoch: 0\n",
      "Loss: 2.025179624557495\n",
      "Epoch: 0\n",
      "Loss: 1.3806618452072144\n",
      "Epoch: 0\n",
      "Loss: 2.694112777709961\n",
      "Epoch: 0\n",
      "Loss: 1.7756913900375366\n",
      "Epoch: 0\n",
      "Loss: 1.3466659784317017\n",
      "Epoch: 0\n",
      "Loss: 1.4673303365707397\n",
      "Epoch: 0\n",
      "Loss: 1.300416350364685\n",
      "Epoch: 0\n",
      "Loss: 1.3392724990844727\n",
      "Epoch: 0\n",
      "Loss: 1.7129039764404297\n",
      "Epoch: 0\n",
      "Loss: 1.3588838577270508\n",
      "Epoch: 0\n",
      "Loss: 1.7274441719055176\n",
      "Epoch: 0\n",
      "Loss: 1.4651519060134888\n",
      "Epoch: 0\n",
      "Loss: 1.4066389799118042\n",
      "Epoch: 0\n",
      "Loss: 1.7007877826690674\n",
      "Epoch: 0\n",
      "Loss: 2.2076916694641113\n",
      "Epoch: 0\n",
      "Loss: 1.2881213426589966\n",
      "Epoch: 0\n",
      "Loss: 1.6889972686767578\n",
      "Epoch: 0\n",
      "Loss: 1.2862662076950073\n",
      "Epoch: 0\n",
      "Loss: 1.0042610168457031\n",
      "Epoch: 0\n",
      "Loss: 1.6195276975631714\n",
      "Epoch: 0\n",
      "Loss: 1.7000917196273804\n",
      "Epoch: 0\n",
      "Loss: 2.749124050140381\n",
      "Epoch: 0\n",
      "Loss: 1.6459563970565796\n",
      "Epoch: 0\n",
      "Loss: 1.7799420356750488\n",
      "Epoch: 0\n",
      "Loss: 2.171551465988159\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([x[\u001b[39m1\u001b[39m:], eos])\n\u001b[1;32m     52\u001b[0m \u001b[39m# run our batch through the whole transformer (attention1, ffw, attention2, ffw, linear)\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m p, _ \u001b[39m=\u001b[39m m(x)\n\u001b[1;32m     54\u001b[0m \u001b[39m# calculate cross-entropy loss between predicted token and target token\u001b[39;00m\n\u001b[1;32m     55\u001b[0m l \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mcross_entropy(p, y)\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[70], line 45\u001b[0m, in \u001b[0;36mBesSimpleTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 45\u001b[0m   emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding(x)\n\u001b[1;32m     46\u001b[0m   pos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_emb[\u001b[39m0\u001b[39m:x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], :]\n\u001b[1;32m     47\u001b[0m   emb \u001b[39m=\u001b[39m emb \u001b[39m+\u001b[39m pos\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "# target_accuracies = [50, 60, 70, 80, 90, 100]\n",
    "# current_target_idx = 0\n",
    "# best_accuracy_so_far = 0\n",
    "\n",
    "# loss_history = []\n",
    "# num_epochs = 100\n",
    "# bb= False\n",
    "# for epoch in range(num_epochs):\n",
    "#   if bb == True:\n",
    "#     break\n",
    "#   for idx, batch in enumerate(dl):\n",
    "#     sos = torch.tensor([2])\n",
    "#     eos = torch.tensor([1])\n",
    "#     x = batch[0]\n",
    "#     x = torch.cat([sos, x])\n",
    "#     y = torch.cat([x[1:], eos])\n",
    "#     p = m(x, 1)\n",
    "#     l = torch.nn.functional.cross_entropy(p, y)\n",
    "#     _, predicted = torch.max(p, 1)\n",
    "#     correct = (predicted == y).sum().item()\n",
    "#     total = y.size(0)\n",
    "#     accuracy = 100 * correct / total\n",
    "#     if accuracy > best_accuracy_so_far:\n",
    "#         if accuracy >= target_accuracies[current_target_idx]:\n",
    "#             save_path = f\"model_{target_accuracies[current_target_idx]}_accuracy.pth\"\n",
    "#             torch.save(m.state_dict(), save_path)\n",
    "#             print(f\"Model saved with accuracy: {accuracy:.2f}% at {save_path}\")\n",
    "\n",
    "#             current_target_idx += 1\n",
    "#             if current_target_idx >= len(target_accuracies):\n",
    "#                     bb = True\n",
    "#                     break\n",
    "loss_history = []\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "  for idx, batch in enumerate(dl):\n",
    "\n",
    "    # sos = torch.tensor([2])\n",
    "    # eos = torch.tensor([1])\n",
    "    # Derive sos and eos from tokenizer\n",
    "    sos = torch.tensor([tokenizer.sp.piece_to_id('<s>')])\n",
    "    eos = torch.tensor([tokenizer.sp.piece_to_id('</s>')])\n",
    "    \n",
    "\n",
    "    # for each row in batch\n",
    "    x = batch[0]\n",
    "    # add sos to beginning of row\n",
    "    x = torch.cat([sos, x])\n",
    "    # In target tensor, add eos to end of each row and remove sos from start\n",
    "    y = torch.cat([x[1:], eos])\n",
    "\n",
    "    # run our batch through the whole transformer (attention1, ffw, attention2, ffw, linear)\n",
    "    p, _ = m(x)\n",
    "    # calculate cross-entropy loss between predicted token and target token\n",
    "    l = torch.nn.functional.cross_entropy(p, y)\n",
    "    # print loss every 1000 rows of dataset\n",
    "    if (idx % 1000 == 0): \n",
    "        print(\"Epoch:\", epoch)\n",
    "        print(\"Loss:\", l.item())\n",
    "        # print(\"Accuracy: {:.2f}%\".format(accuracy))\n",
    "\n",
    "        \n",
    "        # incorrect_indices = (predicted != y).nonzero().squeeze()\n",
    "        # if incorrect_indices.dim() == 0:\n",
    "        #     print(\"All samples are correctly classified!\")\n",
    "        # else:\n",
    "        #     if len(incorrect_indices) > 4:\n",
    "        #         print(\"Misclassified samples:\", tokenizer.decode(incorrect_indices.tolist()))\n",
    "        #         for idx in incorrect_indices[:5]: \n",
    "        #             print(\"True Label:\", y[idx].item(),\":\", tokenizer.decode([y[idx].item()]), \"Predicted Label:\", predicted[idx].item(), \":\", tokenizer.decode([predicted[idx].item()]))\n",
    "\n",
    "    l.backward()\n",
    "  \n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BesSimpleTransformer(\n",
       "  (embedding): Embedding(500, 7)\n",
       "  (layer_00_key): Linear(in_features=7, out_features=11, bias=True)\n",
       "  (layer_00_qry): Linear(in_features=7, out_features=11, bias=True)\n",
       "  (layer_00_val): Linear(in_features=7, out_features=11, bias=True)\n",
       "  (layer_00_ffw): Linear(in_features=11, out_features=7, bias=True)\n",
       "  (layer_01_key): Linear(in_features=7, out_features=11, bias=True)\n",
       "  (layer_01_qry): Linear(in_features=7, out_features=11, bias=True)\n",
       "  (layer_01_val): Linear(in_features=7, out_features=11, bias=True)\n",
       "  (layer_01_ffw): Linear(in_features=11, out_features=7, bias=True)\n",
       "  (map_to_vocab): Linear(in_features=7, out_features=500, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = BesSimpleTransformer()\n",
    "# m.load_state_dict(torch.load(\"model_100_accuracy.pth\"))\n",
    "\n",
    "# m.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "t() expects a tensor with <= 2 dimensions, but self is 3D",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 32\u001b[0m\n\u001b[1;32m     25\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([sos_tensor, encoded_x])\n\u001b[1;32m     29\u001b[0m \u001b[39mwhile\u001b[39;00m goon \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m---> 32\u001b[0m     p_logits \u001b[39m=\u001b[39m m(x, temperature\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     33\u001b[0m     p_probs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(p_logits, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     34\u001b[0m     \u001b[39m# run our random start through transformer \u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[39m#   p_logits = m(x)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \u001b[39m# create probabilities from 29 token options\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[39m#   p_probs = torch.nn.functional.softmax(p_logits, dim=1)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[39m# choose the best prediction (most probable next token according to tranformer)\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[41], line 91\u001b[0m, in \u001b[0;36mBesSimpleTransformer.forward\u001b[0;34m(self, x, temperature)\u001b[0m\n\u001b[1;32m     89\u001b[0m qry \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_00_qry(emb)\n\u001b[1;32m     90\u001b[0m val \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_00_val(emb)\n\u001b[0;32m---> 91\u001b[0m att \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmm(qry, key\u001b[39m.\u001b[39;49mt())\n\u001b[1;32m     92\u001b[0m \u001b[39m# mask from 0 to token end (square mask)\u001b[39;00m\n\u001b[1;32m     93\u001b[0m msk \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask[\u001b[39m0\u001b[39m:x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m0\u001b[39m:x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: t() expects a tensor with <= 2 dimensions, but self is 3D"
     ]
    }
   ],
   "source": [
    "\n",
    "import string\n",
    "\n",
    "goon=True\n",
    "alllookalike=0\n",
    "for x in string.ascii_lowercase:\n",
    "\n",
    "\n",
    "    print(\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\")    \n",
    "    sos_tensor = sos.unsqueeze(0) # if sos is a scalar, this will turn it into a 1D tensor\n",
    "\n",
    "    encoded_x = torch.tensor(tokenizer.encode([x])) # ensure it's a 1D tensor\n",
    "\n",
    "    if encoded_x.dim() == 0:\n",
    "        encoded_x = encoded_x.unsqueeze(0)\n",
    "\n",
    "    x = torch.cat([sos_tensor, encoded_x])\n",
    "\n",
    "    \n",
    "    \n",
    "    while goon == True:\n",
    "        p_logits = m(x, temperature=1)\n",
    "        p_probs = torch.nn.functional.softmax(p_logits, dim=-1)\n",
    "        p_token = torch.argmax(p_probs, dim=1)\n",
    "        predicted_prob = p_probs[0][p_token[-1]].item()\n",
    "        print(\"Input:\", tokenizer.decode(x[1:].tolist()), \"Prediction:\", tokenizer.decode(p_token[-1:].tolist()),f\"Probability: {predicted_prob:.4f}\" )\n",
    "        x = torch.cat([x, p_token[-1].unsqueeze(0)])\n",
    "        if p_token[-1] == 1 or len(p_token.tolist()) == 17: break\n",
    "\n",
    "    print(\"Generate:\", tokenizer.decode(x[1:].tolist()))\n",
    "    generated= tokenizer.decode(x.tolist())\n",
    "\n",
    "    generated = generated.replace('<sos>', '')\n",
    "    generated = generated.replace('<eos>', '')\n",
    "    print(generated)\n",
    "\n",
    "    counter=0\n",
    "    with open('names.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            if generated in line:\n",
    "                counter += 1\n",
    "\n",
    "            if generated == line:\n",
    "                print(\"exact match!:\", line)\n",
    "        print(\"look alike names:\", counter)\n",
    "        temp = counter\n",
    "        alllookalike += temp\n",
    "\n",
    "print(\"Temperature 2, :\" , alllookalike)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
